{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Reuirements**\n",
        "* accelerate==1.9.0\n",
        "* bitsandbytes==0.46.1\n",
        "* transformers==4.54.1\n",
        "\n",
        "**config.json**\n",
        "* {\"HF_TOKEN\": \"Enter your hugging face token\"}\n",
        "\n",
        "* https://huggingface.co/settings/tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "3jv5Hos-dKg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the Dependencies**"
      ],
      "metadata": {
        "id": "J86g4A80VcR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1zT5Mu_Tscz",
        "outputId": "b045b899-4a1e-4c80-dc99-3a3c4d8176e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.9.0)\n",
            "Collecting bitsandbytes==0.46.1 (from -r requirements.txt (line 2))\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers==4.54.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.54.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (0.34.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1->-r requirements.txt (line 3)) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1->-r requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1->-r requirements.txt (line 3)) (0.21.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0->-r requirements.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0->-r requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.1->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.1->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.1->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.54.1->-r requirements.txt (line 3)) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.9.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig,\n",
        "                          pipeline)"
      ],
      "metadata": {
        "id": "n05FZfenUoX3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HF account Configuration**"
      ],
      "metadata": {
        "id": "qNLuH0jeVg9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_data = json.load(open(\"/content/config.json\"))\n",
        "HF_TOKEN = config_data[\"HF_TOKEN\"]"
      ],
      "metadata": {
        "id": "keixzUr0VA0I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.1-8B\""
      ],
      "metadata": {
        "id": "7TzoXD4-VLWq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantisation Configuration**"
      ],
      "metadata": {
        "id": "O6NesmXMVj0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "RygBz42GVR1c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Tokenizer and the LLM**"
      ],
      "metadata": {
        "id": "XdLM7heuVmRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          token=HF_TOKEN)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242,
          "referenced_widgets": [
            "c0ed2313191046a5bacd9c35011d1cc4",
            "7f3fe8274f24467a969400093e33fe9a",
            "04982015b3f34a168107eeb9cf49372f",
            "23f6ff9e721547f5816ba52fdfecf15a",
            "6afe162c43e54269b7e42830aed20836",
            "39d052f4ebf94e499e51afab5d8ccf27",
            "ff6e0de0f2a247bdb45ddf56ae711179",
            "a7decd989c2642e8879a6e4336d30b67",
            "e791ee44d97f4345a4cdf4651997c1fd",
            "f08ed1045fb942c8ba63cfec134cfee6",
            "82c3081566f345d0a761c113eeab4644",
            "0b0b32917cb848ce8cab67c518bc153f",
            "628522e4beaa42218ec9bb1f6d0bffcb",
            "fd68aca6d9bb4697a2bc0bcbea3fdd41",
            "7861ce96367c4a0dab05f987b10960d2",
            "eeb05efead96435ea28f11fcfd1462dc",
            "3b81e01b9fb440459ff4bd5d2ec2d8c7",
            "2e6d4734498243ed9ac0a81351fcf2bb",
            "aa7eb1433282484f8d8430353df4ee12",
            "5f1144fcd26b44c58027ca5516e62890",
            "361da1d8b4194a60b8ab7f66af0af27e",
            "58752cae8a3446f0b943ce13db0f84c9",
            "8853e8eb466d4ee99ce508448d8d34ef",
            "5367d993235b4094a8ddc9ef51fd74d5",
            "be473c3ae2f443fca81300eadf1acb05",
            "7d66ded971a54dfeb4f6124772301b24",
            "a3cd6ec36e6e47a68e38a530c9d888a0",
            "3d57ae14041946fc941d43f7519f8a86",
            "4aa1fc8a6ab14fed9ac3e3421197b8aa",
            "9b21dcd6e51044ad8d9abab8eefce06b",
            "1789d1a1d04740d9bae2901b3907a7a8",
            "ec605186ab87490cb162d0116e04ad19",
            "47c7010af9ef4e27978c1ff0469112f3"
          ]
        },
        "id": "SIEj-iYRVTwR",
        "outputId": "8c272614-199d-42ce-d99f-9f89060054a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0ed2313191046a5bacd9c35011d1cc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b0b32917cb848ce8cab67c518bc153f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8853e8eb466d4ee99ce508448d8d34ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "28a215ccf59d4abe85e0ff683b0d4d69",
            "e98c517465164be0927040f5c287c5fc",
            "2cebca7366784cedbac7a6af3d58e3e4",
            "7fc36e6714c74a548fb26a3dbec3da0f",
            "a16eff0278e44ffb9fe653cbf839260c",
            "947f4185d0d148f58351307e962981fb",
            "9a4fd24868524a0c8148a6708aa9d77b",
            "0e623549d57d4a2bb348e4ea7ba8b1ff",
            "333349c593b64dfe906bbf88e6875125",
            "3b271a8d1c314fe4950c78abf65f05a6",
            "16ca608417884f54a1d278a504ab967e",
            "9a17ec3f6636432e9ab00af85875a758",
            "2ed1bf110a2b4a31b9018781072a552c",
            "3f2856ec2ca9489e8375e0b451cdd06c",
            "209e6ce806c641eeb055d06487e1cf3e",
            "5d395ebf64634c7087673fddb1f895ee",
            "09e8d3eae33f42558943f9ee79f80c87",
            "bed6be28f0134c57abb41db1ba0a80cf",
            "17a304a1431c411b8180c80cdf74d273",
            "5eb1549184054492ba70d12afe0c7385",
            "53f8e39a43a346049d93ea2cf93868e3",
            "d9ebaeb6ddde4b068b5685143a8f45cb",
            "90be6c8d874f4467914880c4f35f44f2",
            "7bc2b4903c6c480e95ac5de5f5e8bf78",
            "e7562290d372428b87c6d19ba7fbf5e5",
            "9d26c9b180954ed0aa0082ebf7953ed9",
            "acb7ce85d562434bb1860eb5a0e6f235",
            "c763c96885f2418aae40d561f98b5596",
            "9d3300462b8f47cb8c90902397b656cf",
            "00ba7bdf4e084b62a458c18bd8a28a53",
            "24b6c5fdaebb4416b90253425d0e937d",
            "54631bd9c1304c4f9df94b9f81265f4d",
            "69825463ee70424291e8aaf6d862d8cb",
            "ffd1a610ab8646ef9f8a08dd2c684410",
            "6155a53f153042108a738eccf0d5fb68",
            "b2f7c3307fe7471187e67622e8383f13",
            "250e222519744ae0bf397d38da214769",
            "415dc56887534f7c8b6563531de2726d",
            "1b8d3573f3724b9197b5f39da58bb7d1",
            "600a6331624e4548b35e69a83cc0c71f",
            "f086414ae14d4051b4c0a6f2e13207e4",
            "febe5a4d76944b53ac54ba802f25d02f",
            "00aacffc11a44d369266fe72c86af690",
            "f2364ca1d66243858e932373d4fbcdb5",
            "3b9a91f41d5c4a10a0d4b879b7f924c5",
            "25ea31446a264e0bb4f452708d0e11e9",
            "31eb25de9721492f950f079d7be76cf4",
            "e942be399e59467ea39f90c0a4f2da53",
            "fb1fd1f8ef814ec788d38fd072c64bc9",
            "5750f3a6f5e9420cb372980fbc99028d",
            "c9cc5a2d2dc44addabb2c1d303187785",
            "ad5e9d41234f40e2869910bd61a967fd",
            "0866a4700712443db34e5dd98e9dfcc8",
            "62ba8053cbf243028a4b242530ae0cc5",
            "bf32e3dbff1c480db4a0cbbd80a48a54",
            "52754311935b491ca3c377c3ef2483ee",
            "4f0a1d734a5e42069d4a00528d3c20d4",
            "d8ba6cebb7394e55a705cf53f911f79c",
            "d91717da81e2454a85b36994311577c4",
            "da5950caf56d43e980819646a65527d7",
            "ddf46ea8848a4ba98baf358860b122c7",
            "7864ecd6e03d44ec9b05126a9d98220d",
            "78ef3457aa7746b6bee984873d5cc1bc",
            "2c33b3ac0c0d4eff837396228c0ef39e",
            "9499fb75c17c46419b478287afcb77a4",
            "6ce5497a9c9a4a728b4b97877a230afa",
            "624f34bc393443078d6af7267f11016f",
            "ec8f606c244144d085081f425d05cc35",
            "261e6a9da7734fe392d3c5246a994b97",
            "c9ac40d5c0ff485f84ebfac84e8d706b",
            "224c419f54a4431b9a6787d84cde7da9",
            "338a639a83534cf5af3e2e2e074ea1c4",
            "ae54ddccf42546c6b8590299d1f68846",
            "f0d98dd00ffb4574a9ff73516224bb74",
            "e432e3bfb5e549be960d6c3f6d197c56",
            "7cacef8419274a9b95e95652e3b94b4c",
            "7825dc4bc5fc4c918ff7dc07d0ae1315",
            "e5060dcbd2ad4dba88c024f16c7d3d40",
            "e406090d41b94ff888203dea6cfdf9c1",
            "b9e5a066d87840f19cf5e43526d381ba",
            "cbca9d70b3ff4fefa399c90e16d41545",
            "359a196ec37f4cd5b2751e99833ded7b",
            "3a7f8b1e76274551aadac7cf3c045e8c",
            "6b09d8bb203146ff99bacb337d1ae5df",
            "5450ce3414064646827b7df95b98a90d",
            "eebd959befec4423956e56eda413c471",
            "ea395fd8cc3346b2b80938aa3bf2ed64",
            "a2277ddf933649eda934afd98d75fefe",
            "97b1e027076a4ae4a9380008d91ffcff",
            "2eb506f7889a49e38b4d51769c8d0d1d",
            "f1beedc2756442a888e9b8a3e2197e94",
            "0a5df7bb4ad145cd8afdbb6246c1b3c9",
            "3e2d78829825457389e2b7062ee7c323",
            "e613dd807b7c4149a4ebac9b8db0b456",
            "d4a5f5bc6f3944a9986b7df0b541c3c6",
            "9d2b5fa01cc14bc7bfb7f407ca6b5da2",
            "fa82d423d6744018b51ae1a9438a5b86",
            "f9d2bacfc7d840e5b25767098a0aa17d",
            "40cf7f07d2a0403b8160b3ee717cd58f"
          ]
        },
        "id": "tFEz-fvSVWSW",
        "outputId": "b89f0909-f6e5-4def-b878-6450e3470e74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28a215ccf59d4abe85e0ff683b0d4d69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a17ec3f6636432e9ab00af85875a758"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90be6c8d874f4467914880c4f35f44f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffd1a610ab8646ef9f8a08dd2c684410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b9a91f41d5c4a10a0d4b879b7f924c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52754311935b491ca3c377c3ef2483ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "624f34bc393443078d6af7267f11016f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5060dcbd2ad4dba88c024f16c7d3d40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97b1e027076a4ae4a9380008d91ffcff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-R-lv6RVYpO",
        "outputId": "5e3282fc-5bba-4753-f23c-2de8a8606e68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt):\n",
        "  sequences = text_generator(prompt)\n",
        "  gen_text = sequences[0][\"generated_text\"]\n",
        "  return gen_text"
      ],
      "metadata": {
        "id": "_YFCOU-dWAcF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Iphone 15 specification?\""
      ],
      "metadata": {
        "id": "XCDyupeBWCc6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama3_response = get_response(prompt)"
      ],
      "metadata": {
        "id": "t0RcAo0MWEKN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama3_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ao2VRMfaWSPO",
        "outputId": "3a66082c-caa7-419f-9c7f-733bd5f75e84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Iphone 15 specification? Here are the latest rumors!\\nThe iPhone 15 Pro and iPhone 15 Pro Max are rumored to be the first iPhone models to feature a titanium frame and a periscope telephoto camera. The iPhone 15 Pro Max is expected to have a 6.7-inch display, while the iPhone 15 Pro is expected to have a 6.1-inch display. Both models are expected to have a triple-lens rear camera setup and a quad-lens front camera setup. The iPhone 15 Pro and iPhone 15 Pro Max are expected to be released in September 2023.\\nThe iPhone 15 Pro and iPhone 15'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ScwWwU_ObmSr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}